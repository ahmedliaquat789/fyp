{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedliaquat789/fyp/blob/master/Diffutoon_jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac0bba5-88c5-46f5-d36f-add315460370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DiffSynth-Studio'...\n",
            "remote: Enumerating objects: 2320, done.\u001b[K\n",
            "remote: Counting objects: 100% (890/890), done.\u001b[K\n",
            "remote: Compressing objects: 100% (351/351), done.\u001b[K\n",
            "remote: Total 2320 (delta 631), reused 622 (delta 530), pack-reused 1430 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2320/2320), 6.11 MiB | 20.38 MiB/s, done.\n",
            "Resolving deltas: 100% (1468/1468), done.\n",
            "/content/DiffSynth-Studio\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for controlnet-aux (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "78708d|\u001b[1;32mOK\u001b[0m  |   182MiB/s|/content/DiffSynth-Studio/models/stable_diffusion/aingdiffusion_v12.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "338aaa|\u001b[1;32mOK\u001b[0m  |   149MiB/s|/content/DiffSynth-Studio/models/AnimateDiff/mm_sd_v15_v2.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "f63db0|\u001b[1;32mOK\u001b[0m  |   185MiB/s|/content/DiffSynth-Studio/models/ControlNet/control_v11p_sd15_lineart.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "7b519e|\u001b[1;32mOK\u001b[0m  |   186MiB/s|/content/DiffSynth-Studio/models/ControlNet/control_v11f1e_sd15_tile.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "12f0f9|\u001b[1;32mOK\u001b[0m  |   102MiB/s|/content/DiffSynth-Studio/models/ControlNet/control_v11f1p_sd15_depth.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "428cb8|\u001b[1;32mOK\u001b[0m  |   104MiB/s|/content/DiffSynth-Studio/models/ControlNet/control_v11p_sd15_softedge.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "82b7cb|\u001b[1;32mOK\u001b[0m  |   189MiB/s|/content/DiffSynth-Studio/models/Annotators/dpt_hybrid-midas-501f0c75.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "f4ae5e|\u001b[1;32mOK\u001b[0m  |    73MiB/s|/content/DiffSynth-Studio/models/Annotators/ControlNetHED.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "bc4af0|\u001b[1;32mOK\u001b[0m  |    46MiB/s|/content/DiffSynth-Studio/models/Annotators/sk_model.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "a073ca|\u001b[1;32mOK\u001b[0m  |    43MiB/s|/content/DiffSynth-Studio/models/Annotators/sk_model2.pth\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "70f935|\u001b[1;32mOK\u001b[0m  |   3.0MiB/s|/content/DiffSynth-Studio/models/textual_inversion/verybadimagenegative_v1.3.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "843947|\u001b[1;32mOK\u001b[0m  |    34MiB/s|/content/input_video.mp4\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/modelscope/DiffSynth-Studio/blob/main/examples/Diffutoon/Diffutoon.ipynb modified\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Artiprocher/DiffSynth-Studio\n",
        "%cd /content/DiffSynth-Studio\n",
        "\n",
        "!pip install -q einops transformers controlnet-aux==0.0.7 sentencepiece imageio imageio-ffmpeg\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/229575 -d /content/DiffSynth-Studio/models/stable_diffusion -o aingdiffusion_v12.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/guoyww/animatediff/resolve/main/mm_sd_v15_v2.ckpt -d /content/DiffSynth-Studio/models/AnimateDiff -o mm_sd_v15_v2.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_lineart.pth -d /content/DiffSynth-Studio/models/ControlNet -o control_v11p_sd15_lineart.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1e_sd15_tile.pth -d /content/DiffSynth-Studio/models/ControlNet -o control_v11f1e_sd15_tile.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth -d /content/DiffSynth-Studio/models/ControlNet -o control_v11f1p_sd15_depth.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_softedge.pth -d /content/DiffSynth-Studio/models/ControlNet -o control_v11p_sd15_softedge.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/Annotators/resolve/main/dpt_hybrid-midas-501f0c75.pt -d /content/DiffSynth-Studio/models/Annotators -o dpt_hybrid-midas-501f0c75.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/Annotators/resolve/main/ControlNetHED.pth -d /content/DiffSynth-Studio/models/Annotators -o ControlNetHED.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/Annotators/resolve/main/sk_model.pth -d /content/DiffSynth-Studio/models/Annotators -o sk_model.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/Annotators/resolve/main/sk_model2.pth -d /content/DiffSynth-Studio/models/Annotators -o sk_model2.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://civitai.com/api/download/models/25820?type=Model&format=PickleTensor&size=full&fp=fp16\" -d /content/DiffSynth-Studio/models/textual_inversion -o verybadimagenegative_v1.3.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/Diffutoon/resolve/main/input_video.mp4 -d /content -o input_video.mp4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqF6NT5UIUeR",
        "outputId": "303ae934-5b98-4093-d6ed-6296d280d135"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "npsb8kdEIRG2"
      },
      "outputs": [],
      "source": [
        "config_stage_2_template = {\n",
        "    \"models\": {\n",
        "        \"model_list\": [\n",
        "            \"models/stable_diffusion/aingdiffusion_v12.safetensors\",\n",
        "            \"models/AnimateDiff/mm_sd_v15_v2.ckpt\",\n",
        "            \"models/ControlNet/control_v11f1e_sd15_tile.pth\",\n",
        "            \"models/ControlNet/control_v11p_sd15_lineart.pth\"\n",
        "        ],\n",
        "        \"textual_inversion_folder\": \"models/textual_inversion\",\n",
        "        \"device\": \"cuda\",\n",
        "        \"lora_alphas\": [],\n",
        "        \"controlnet_units\": [\n",
        "            {\n",
        "                \"processor_id\": \"tile\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11f1e_sd15_tile.pth\",\n",
        "                \"scale\": 0.5\n",
        "            },\n",
        "            {\n",
        "                \"processor_id\": \"lineart\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11p_sd15_lineart.pth\",\n",
        "                \"scale\": 0.5\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"input_frames\": {\n",
        "            \"video_file\": \"/content/IMG_1572.mp4\",\n",
        "            \"image_folder\": None,\n",
        "            \"height\": 960,\n",
        "            \"width\": 540,\n",
        "            \"start_frame_id\": 0,\n",
        "            \"end_frame_id\": 193\n",
        "        },\n",
        "        \"controlnet_frames\": [\n",
        "            {\n",
        "                \"video_file\": \"/content/IMG_1572.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 960,\n",
        "                \"width\": 540,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 193\n",
        "            },\n",
        "            {\n",
        "                \"video_file\": \"/content/IMG_1572.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 960,\n",
        "                \"width\": 540,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 193\n",
        "            }\n",
        "        ],\n",
        "        \"output_folder\": \"/content/output\",\n",
        "        \"fps\": 25\n",
        "    },\n",
        "    \"pipeline\": {\n",
        "        \"seed\": 0,\n",
        "        \"pipeline_inputs\": {\n",
        "            \"prompt\": \"best quality, perfect anime illustration\",\n",
        "            \"negative_prompt\": \"verybadimagenegative_v1.3\",\n",
        "            \"cfg_scale\": 7.0,\n",
        "            \"clip_skip\": 2,\n",
        "            \"denoising_strength\": 1.0,\n",
        "            \"num_inference_steps\": 10,\n",
        "            \"animatediff_batch_size\": 16,\n",
        "            \"animatediff_stride\": 8,\n",
        "            \"unet_batch_size\": 1,\n",
        "            \"controlnet_batch_size\": 1,\n",
        "            \"cross_frame_attention\": False,\n",
        "            # The following parameters will be overwritten. You don't need to modify them.\n",
        "            \"input_frames\": [],\n",
        "            \"num_frames\": 30,\n",
        "            \"width\": 540,\n",
        "            \"height\": 960,\n",
        "            \"controlnet_frames\": []\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelscope"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx2kOFczKZc4",
        "outputId": "9c87d21b-d95a-44f7-cf00-be1cfa86cbf5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modelscope\n",
            "  Downloading modelscope-1.21.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.25 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from modelscope) (4.66.6)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (2024.8.30)\n",
            "Downloading modelscope-1.21.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: modelscope\n",
            "Successfully installed modelscope-1.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "P9dIs620IRG2",
        "outputId": "e6867236-3850-4f3f-8a47-507396e6e105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:diffsynth.models.kolors_text_encoder:Failed to load cpm_kernels:No module named 'cpm_kernels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models from: models/stable_diffusion/aingdiffusion_v12.safetensors\n",
            "    model_name: sd_text_encoder model_class: SDTextEncoder\n",
            "    model_name: sd_unet model_class: SDUNet\n",
            "    model_name: sd_vae_decoder model_class: SDVAEDecoder\n",
            "    model_name: sd_vae_encoder model_class: SDVAEEncoder\n",
            "    The following models are loaded: ['sd_text_encoder', 'sd_unet', 'sd_vae_decoder', 'sd_vae_encoder'].\n",
            "Loading models from: models/AnimateDiff/mm_sd_v15_v2.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/DiffSynth-Studio/diffsynth/models/utils.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(file_path, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    model_name: sd_motion_modules model_class: SDMotionModel\n",
            "    The following models are loaded: ['sd_motion_modules'].\n",
            "Loading models from: models/ControlNet/control_v11f1e_sd15_tile.pth\n",
            "    model_name: sd_controlnet model_class: SDControlNet\n",
            "    The following models are loaded: ['sd_controlnet'].\n",
            "Loading models from: models/ControlNet/control_v11p_sd15_lineart.pth\n",
            "    model_name: sd_controlnet model_class: SDControlNet\n",
            "    The following models are loaded: ['sd_controlnet'].\n",
            "Using sd_text_encoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_unet from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_vae_decoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_vae_encoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_controlnet from models/ControlNet/control_v11f1e_sd15_tile.pth.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/lineart/__init__.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/lineart/__init__.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  coarse_model.load_state_dict(torch.load(coarse_model_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sd_controlnet from models/ControlNet/control_v11p_sd15_lineart.pth.\n",
            "No sd_ipadapter models available.\n",
            "No sd_ipadapter_clip_image_encoder models available.\n",
            "Using sd_motion_modules from models/AnimateDiff/mm_sd_v15_v2.ckpt.\n",
            "The width cannot be evenly divided by 64. We round it up to 576.\n",
            "Textual inversion verybadimagenegative_v1.3 is enabled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 193/193 [00:01<00:00, 139.61it/s]\n",
            "100%|██████████| 193/193 [00:32<00:00,  5.90it/s]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (72) must match the size of tensor b (68) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-232bcb555b8c>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDVideoPipelineRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/pipeline_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0msmoother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_streamlit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Synthesizing videos ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0moutput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesize_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline_inputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_streamlit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Synthesizing videos ... done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_streamlit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving videos ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/pipeline_runner.py\u001b[0m in \u001b[0;36msynthesize_video\u001b[0;34m(self, model_manager, pipe, seed, smoother, **pipeline_inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprogress_bar_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpipeline_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mmodel_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/sd_video.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, negative_prompt, cfg_scale, clip_skip, num_frames, input_frames, ipadapter_images, ipadapter_scale, controlnet_frames, denoising_strength, height, width, num_inference_steps, animatediff_batch_size, animatediff_stride, unet_batch_size, controlnet_batch_size, cross_frame_attention, smoother, smoother_progress_ids, tiled, tile_size, tile_stride, seed, progress_bar_cmd, progress_bar_st)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# Classifier-free guidance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             noise_pred_posi = lets_dance_with_long_video(\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_modules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotion_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrolnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/sd_video.py\u001b[0m in \u001b[0;36mlets_dance_with_long_video\u001b[0;34m(unet, motion_modules, controlnet, sample, timestep, encoder_hidden_states, ipadapter_kwargs_list, controlnet_frames, unet_batch_size, controlnet_batch_size, cross_frame_attention, tiled, tile_size, tile_stride, device, animatediff_batch_size, animatediff_stride)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# process this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         hidden_states_batch = lets_dance(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_id_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/dancer.py\u001b[0m in \u001b[0;36mlets_dance\u001b[0;34m(unet, motion_modules, controlnet, sample, timestep, encoder_hidden_states, ipadapter_kwargs_list, controlnet_frames, unet_batch_size, controlnet_batch_size, cross_frame_attention, tiled, tile_size, tile_stride, device, vram_limit_level)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolnet_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbatch_id_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontrolnet_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             res_stack = controlnet(\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_id_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/controlnets/controlnet_unit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample, timestep, encoder_hidden_states, conditionings, tiled, tile_size, tile_stride, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mres_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditionings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             res_stack_ = model(\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/models/sd_controlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, conditioning, tiled, tile_size, tile_stride, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# 2. pre-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrolnet_conv_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditioning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mres_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (72) must match the size of tensor b (68) at non-singleton dimension 3"
          ]
        }
      ],
      "source": [
        "from diffsynth import SDVideoPipelineRunner\n",
        "\n",
        "config = config_stage_2_template.copy()\n",
        "config[\"data\"][\"input_frames\"] = {\n",
        "    \"video_file\": \"/content/input_video.mp4\",\n",
        "    \"image_folder\": None,\n",
        "    \"height\": 960,\n",
        "    \"width\": 540,\n",
        "    \"start_frame_id\": 0,\n",
        "    \"end_frame_id\": 193\n",
        "}\n",
        "config[\"data\"][\"controlnet_frames\"] = [config[\"data\"][\"input_frames\"], config[\"data\"][\"input_frames\"]]\n",
        "config[\"data\"][\"output_folder\"] = \"/content/toon_video\"\n",
        "config[\"data\"][\"fps\"] = 25\n",
        "\n",
        "runner = SDVideoPipelineRunner()\n",
        "runner.run(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r2jfNGIIRG3"
      },
      "outputs": [],
      "source": [
        "import moviepy.editor\n",
        "moviepy.editor.ipython_display(\"/content/toon_video/video.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffsynth import SDVideoPipelineRunner\n",
        "\n",
        "# Corrected configuration\n",
        "config = {\n",
        "    \"models\": {\n",
        "        \"model_list\": [\n",
        "            \"models/stable_diffusion/aingdiffusion_v12.safetensors\",\n",
        "            \"models/AnimateDiff/mm_sd_v15_v2.ckpt\",\n",
        "            \"models/ControlNet/control_v11f1e_sd15_tile.pth\",\n",
        "            \"models/ControlNet/control_v11p_sd15_lineart.pth\"\n",
        "        ],\n",
        "        \"textual_inversion_folder\": \"models/textual_inversion\",\n",
        "        \"device\": \"cuda\",\n",
        "        \"lora_alphas\": [],\n",
        "        \"controlnet_units\": [\n",
        "            {\n",
        "                \"processor_id\": \"tile\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11f1e_sd15_tile.pth\",\n",
        "                \"scale\": 1.0  # Ensuring uniform scale\n",
        "            },\n",
        "            {\n",
        "                \"processor_id\": \"lineart\",\n",
        "                \"model_path\": \"models/ControlNet/control_v11p_sd15_lineart.pth\",\n",
        "                \"scale\": 1.0\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"input_frames\": {\n",
        "            \"video_file\": \"/content/IMG_1572.mp4\",\n",
        "            \"image_folder\": None,\n",
        "            \"height\": 960,  # Already divisible by 64\n",
        "            \"width\": 576,   # Adjusted to be divisible by 64\n",
        "            \"start_frame_id\": 0,\n",
        "            \"end_frame_id\": 193\n",
        "        },\n",
        "        \"controlnet_frames\": [\n",
        "            {\n",
        "                \"video_file\": \"/content/IMG_1572.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 960,\n",
        "                \"width\": 576,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 193\n",
        "            },\n",
        "            {\n",
        "                \"video_file\": \"/content/IMG_1572.mp4\",\n",
        "                \"image_folder\": None,\n",
        "                \"height\": 960,\n",
        "                \"width\": 576,\n",
        "                \"start_frame_id\": 0,\n",
        "                \"end_frame_id\": 193\n",
        "            }\n",
        "        ],\n",
        "        \"output_folder\": \"/content/toon_video\",\n",
        "        \"fps\": 25\n",
        "    },\n",
        "    \"pipeline\": {\n",
        "        \"seed\": 0,\n",
        "        \"pipeline_inputs\": {\n",
        "            \"prompt\": \"best quality, perfect anime illustration\",\n",
        "            \"negative_prompt\": \"verybadimagenegative_v1.3\",\n",
        "            \"cfg_scale\": 7.0,\n",
        "            \"clip_skip\": 2,\n",
        "            \"denoising_strength\": 1.0,\n",
        "            \"num_inference_steps\": 10,\n",
        "            \"animatediff_batch_size\": 16,\n",
        "            \"animatediff_stride\": 8,\n",
        "            \"unet_batch_size\": 1,\n",
        "            \"controlnet_batch_size\": 1,\n",
        "            \"cross_frame_attention\": False,\n",
        "            # The following parameters will be overwritten during runtime\n",
        "            \"input_frames\": [],\n",
        "            \"num_frames\": 30,\n",
        "            \"width\": 576,\n",
        "            \"height\": 960,\n",
        "            \"controlnet_frames\": []\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize and run the pipeline\n",
        "runner = SDVideoPipelineRunner()\n",
        "runner.run(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "3MP8PjutMLr7",
        "outputId": "9f22d3d2-dab4-43a2-d94a-5c36716ac37d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models from: models/stable_diffusion/aingdiffusion_v12.safetensors\n",
            "    model_name: sd_text_encoder model_class: SDTextEncoder\n",
            "    model_name: sd_unet model_class: SDUNet\n",
            "    model_name: sd_vae_decoder model_class: SDVAEDecoder\n",
            "    model_name: sd_vae_encoder model_class: SDVAEEncoder\n",
            "    The following models are loaded: ['sd_text_encoder', 'sd_unet', 'sd_vae_decoder', 'sd_vae_encoder'].\n",
            "Loading models from: models/AnimateDiff/mm_sd_v15_v2.ckpt\n",
            "    model_name: sd_motion_modules model_class: SDMotionModel\n",
            "    The following models are loaded: ['sd_motion_modules'].\n",
            "Loading models from: models/ControlNet/control_v11f1e_sd15_tile.pth\n",
            "    model_name: sd_controlnet model_class: SDControlNet\n",
            "    The following models are loaded: ['sd_controlnet'].\n",
            "Loading models from: models/ControlNet/control_v11p_sd15_lineart.pth\n",
            "    model_name: sd_controlnet model_class: SDControlNet\n",
            "    The following models are loaded: ['sd_controlnet'].\n",
            "Using sd_text_encoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_unet from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_vae_decoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_vae_encoder from models/stable_diffusion/aingdiffusion_v12.safetensors.\n",
            "Using sd_controlnet from models/ControlNet/control_v11f1e_sd15_tile.pth.\n",
            "Using sd_controlnet from models/ControlNet/control_v11p_sd15_lineart.pth.\n",
            "No sd_ipadapter models available.\n",
            "No sd_ipadapter_clip_image_encoder models available.\n",
            "Using sd_motion_modules from models/AnimateDiff/mm_sd_v15_v2.ckpt.\n",
            "The width cannot be evenly divided by 64. We round it up to 576.\n",
            "Textual inversion verybadimagenegative_v1.3 is enabled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 193/193 [00:01<00:00, 136.68it/s]\n",
            "100%|██████████| 193/193 [00:33<00:00,  5.72it/s]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (72) must match the size of tensor b (68) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cf851164853e>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Initialize and run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDVideoPipelineRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/pipeline_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0msmoother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_streamlit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Synthesizing videos ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0moutput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesize_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipeline_inputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_streamlit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Synthesizing videos ... done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_streamlit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving videos ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/pipeline_runner.py\u001b[0m in \u001b[0;36msynthesize_video\u001b[0;34m(self, model_manager, pipe, seed, smoother, **pipeline_inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mprogress_bar_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutput_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpipeline_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mmodel_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/sd_video.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, negative_prompt, cfg_scale, clip_skip, num_frames, input_frames, ipadapter_images, ipadapter_scale, controlnet_frames, denoising_strength, height, width, num_inference_steps, animatediff_batch_size, animatediff_stride, unet_batch_size, controlnet_batch_size, cross_frame_attention, smoother, smoother_progress_ids, tiled, tile_size, tile_stride, seed, progress_bar_cmd, progress_bar_st)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# Classifier-free guidance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             noise_pred_posi = lets_dance_with_long_video(\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_modules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotion_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrolnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/sd_video.py\u001b[0m in \u001b[0;36mlets_dance_with_long_video\u001b[0;34m(unet, motion_modules, controlnet, sample, timestep, encoder_hidden_states, ipadapter_kwargs_list, controlnet_frames, unet_batch_size, controlnet_batch_size, cross_frame_attention, tiled, tile_size, tile_stride, device, animatediff_batch_size, animatediff_stride)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# process this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         hidden_states_batch = lets_dance(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_id_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/pipelines/dancer.py\u001b[0m in \u001b[0;36mlets_dance\u001b[0;34m(unet, motion_modules, controlnet, sample, timestep, encoder_hidden_states, ipadapter_kwargs_list, controlnet_frames, unet_batch_size, controlnet_batch_size, cross_frame_attention, tiled, tile_size, tile_stride, device, vram_limit_level)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrolnet_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbatch_id_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontrolnet_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             res_stack = controlnet(\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_id_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/controlnets/controlnet_unit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample, timestep, encoder_hidden_states, conditionings, tiled, tile_size, tile_stride, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mres_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditionings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             res_stack_ = model(\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditioning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtile_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtile_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DiffSynth-Studio/diffsynth/models/sd_controlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, conditioning, tiled, tile_size, tile_stride, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# 2. pre-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrolnet_conv_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditioning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mres_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (72) must match the size of tensor b (68) at non-singleton dimension 3"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}